# Pipeline Failure Handling - How Reports Are Generated

## Quick Answer: YES! âœ…

**Reports are ALWAYS generated and committed to Git, even when tests fail.**

The pipeline is designed to capture failure reports so you can see what went wrong.

---

## How It Works

### 1ï¸âƒ£ Pre-Check Stage

**What happens when pre-check FAILS:**

```
1. Robot tests run â†’ Some tests FAIL âŒ
2. ExecutionCounter tracks the failures âœ…
3. merge_reports.py creates combined history âœ…
4. Git commit happens with message:
   "Pre-check FAILED: cabgridmgr.amfam.com - a_record [Pipeline: 12345] [Total Executions: 47]" âœ…
5. Reports pushed to Git repository âœ…
6. Stage exits with code 1 (failure) âŒ
7. Pipeline STOPS - does NOT proceed to deploy â›”
8. Reporting stage STILL RUNS (see below) âœ…
```

**Files committed to Git (even on failure):**
```
âœ… infoblox_mvp1/robot_reports/pre_check/output.xml
âœ… infoblox_mvp1/robot_reports/pre_check/log.html
âœ… infoblox_mvp1/robot_reports/pre_check/report.html
âœ… infoblox_mvp1/robot_reports/pre_check/combined_report.html â­
âœ… infoblox_mvp1/robot_reports/pre_check/history/output_*.xml
âœ… infoblox_mvp1/robot_reports/execution_counters/pre_check_counter.json
```

**Configuration that makes this work:**
```yaml
artifacts:
  when: always  # â† Artifacts saved even on failure
```

---

### 2ï¸âƒ£ Post-Implementation Stage

**What happens when post-check FAILS:**

```
1. Robot tests run â†’ Some tests FAIL âŒ
2. ExecutionCounter tracks the failures âœ…
3. merge_reports.py creates combined history âœ…
4. Git commit happens with message:
   "Post-implementation FAILED: cabgridmgr.amfam.com - a_record - add [Pipeline: 12345] [Total Executions: 52]" âœ…
5. Reports pushed to Git repository âœ…
6. Stage exits with code 1 (failure) âŒ
7. Reporting stage STILL RUNS (see below) âœ…
```

**Files committed to Git (even on failure):**
```
âœ… infoblox_mvp1/robot_reports/post_check/output.xml
âœ… infoblox_mvp1/robot_reports/post_check/log.html
âœ… infoblox_mvp1/robot_reports/post_check/report.html
âœ… infoblox_mvp1/robot_reports/post_check/combined_report.html â­
âœ… infoblox_mvp1/robot_reports/post_check/history/output_*.xml
âœ… infoblox_mvp1/robot_reports/execution_counters/post_check_counter.json
```

---

### 3ï¸âƒ£ Reporting Stage (ALWAYS RUNS!)

**Configuration:**
```yaml
generate_execution_report:
  stage: reporting
  when: always  # â† Runs even if previous stages failed!
```

**What happens:**

| Scenario | Reporting Stage Behavior |
|----------|------------------------|
| **Pre-check fails** | âœ… RUNS (generates summary with pre-check failure data) |
| **Post-check fails** | âœ… RUNS (generates summary with post-check failure data) |
| **Both fail** | âœ… RUNS (generates summary showing both failures) |
| **Both succeed** | âœ… RUNS (generates summary showing all passes) |

**Files committed to Git (always):**
```
âœ… infoblox_mvp1/robot_reports/execution_summary.html
```

---

## Complete Failure Scenarios

### Scenario A: Pre-Check Fails

```
Pipeline Flow:
â”œâ”€ build-container âœ…
â”œâ”€ file-processing âœ…
â”œâ”€ pre-implementation
â”‚  â”œâ”€ pre_check âŒ FAILS
â”‚  â”‚  â”œâ”€ Tests run (some fail)
â”‚  â”‚  â”œâ”€ Reports generated
â”‚  â”‚  â”œâ”€ Git commit: "Pre-check FAILED: ..." âœ…
â”‚  â”‚  â””â”€ Exit code 1
â”‚  â”‚
â”‚  â”œâ”€ json_validation â›” SKIPPED (pre_check failed)
â”‚  â””â”€ server_connection_test â›” SKIPPED
â”‚
â”œâ”€ deploy â›” SKIPPED (validation failed - SAFE!)
â”œâ”€ post-implementation â›” SKIPPED
â”‚
â””â”€ reporting
   â””â”€ generate_execution_report âœ… RUNS!
      â”œâ”€ Reads pre_check_counter.json (shows failure)
      â”œâ”€ Generates execution_summary.html
      â””â”€ Git commit: "Update execution statistics report" âœ…

Pipeline Status: âŒ FAILED (but reports committed!)
```

---

### Scenario B: Pre-Check Passes, Post-Check Fails

```
Pipeline Flow:
â”œâ”€ build-container âœ…
â”œâ”€ file-processing âœ…
â”œâ”€ pre-implementation
â”‚  â”œâ”€ pre_check âœ… PASSES
â”‚  â”‚  â””â”€ Git commit: "Pre-check PASSED: ..." âœ…
â”‚  â”œâ”€ json_validation âœ…
â”‚  â””â”€ server_connection_test âœ…
â”‚
â”œâ”€ deploy âœ… (validation passed, safe to deploy)
â”‚
â”œâ”€ post-implementation
â”‚  â””â”€ post_implementation âŒ FAILS
â”‚     â”œâ”€ Tests run (some fail)
â”‚     â”œâ”€ Reports generated
â”‚     â”œâ”€ Git commit: "Post-implementation FAILED: ..." âœ…
â”‚     â””â”€ Exit code 1
â”‚
â””â”€ reporting
   â””â”€ generate_execution_report âœ… RUNS!
      â”œâ”€ Reads both counters (pre: pass, post: fail)
      â”œâ”€ Generates execution_summary.html
      â””â”€ Git commit: "Update execution statistics report" âœ…

Pipeline Status: âŒ FAILED (but all reports committed!)
```

---

### Scenario C: Both Pass

```
Pipeline Flow:
â”œâ”€ build-container âœ…
â”œâ”€ file-processing âœ…
â”œâ”€ pre-implementation
â”‚  â”œâ”€ pre_check âœ… PASSES
â”‚  â”‚  â””â”€ Git commit: "Pre-check PASSED: ..." âœ…
â”‚  â”œâ”€ json_validation âœ…
â”‚  â””â”€ server_connection_test âœ…
â”‚
â”œâ”€ deploy âœ…
â”‚
â”œâ”€ post-implementation
â”‚  â””â”€ post_implementation âœ… PASSES
â”‚     â”œâ”€ Tests pass
â”‚     â”œâ”€ Reports generated
â”‚     â””â”€ Git commit: "Post-implementation PASSED: ..." âœ…
â”‚
â””â”€ reporting
   â””â”€ generate_execution_report âœ… RUNS!
      â”œâ”€ Reads both counters (both passed)
      â”œâ”€ Generates execution_summary.html
      â””â”€ Git commit: "Update execution statistics report" âœ…

Pipeline Status: âœ… SUCCESS (all reports committed!)
```

---

## Key Configuration Settings

### Pre-Check Stage
```yaml
pre_check:
  stage: pre-implementation
  artifacts:
    when: always        # â† Save artifacts even on failure
  # No allow_failure    # â† Pipeline stops if this fails (SAFE!)
```

**Why no `allow_failure`?**
- We DON'T want to deploy if pre-check fails
- Pipeline MUST stop to prevent bad changes
- But reports are still saved and committed

---

### Post-Implementation Stage
```yaml
post_implementation:
  stage: post-implementation
  artifacts:
    when: always        # â† Save artifacts even on failure
  # No allow_failure    # â† Mark pipeline as failed if tests fail
```

**Why no `allow_failure`?**
- We need to know if post-check failed
- Pipeline status shows failure (alerts team)
- But reports are still saved and committed

---

### Reporting Stage
```yaml
generate_execution_report:
  stage: reporting
  when: always          # â† ALWAYS run, even if previous stages failed!
  artifacts:
    when: always        # â† Save artifacts even on failure
```

**Why `when: always`?**
- We ALWAYS want execution statistics
- Even failures need to be tracked
- Historical data must be complete

---

## Git Commits on Failure

### Commit Message Examples

**Pre-check failure:**
```
Pre-check FAILED: cabgridmgr.amfam.com - a_record [Pipeline: 12345] [Total Executions: 47]
```

**Post-check failure:**
```
Post-implementation FAILED: cabgridmgr.amfam.com - a_record - add [Pipeline: 12345] [Total Executions: 52]
```

**Reporting (always):**
```
Update execution statistics report [Pipeline: 12345]
```

---

## Where to Find Failure Reports

### In Git Repository (Permanent)

**After pre-check failure:**
```
Repository â†’ infoblox_mvp1/robot_reports/pre_check/
â”œâ”€â”€ combined_report.html â­ (shows failure in history)
â”œâ”€â”€ log.html (detailed failure logs)
â””â”€â”€ report.html (current run failure)
```

**After post-check failure:**
```
Repository â†’ infoblox_mvp1/robot_reports/post_check/
â”œâ”€â”€ combined_report.html â­ (shows failure in history)
â”œâ”€â”€ log.html (detailed failure logs)
â””â”€â”€ report.html (current run failure)
```

**Execution summary (always):**
```
Repository â†’ infoblox_mvp1/robot_reports/execution_summary.html
(Shows all executions, including failures)
```

---

### In GitLab Artifacts (Temporary - 1 week)

```
Pipeline â†’ Job â†’ Browse Artifacts
â””â”€ Same files as in Git repository
```

---

## Code Implementation

### How Reports Are Committed Even on Failure

```bash
# In pre_check stage (lines 340-462)

# Run Robot tests and capture exit code
robot ... || robot_exit_code=$?

# Generate reports (happens BEFORE exit)
python3 -c "from ExecutionCounter import update_counter; ..."
python3 -c "from merge_reports import merge_robot_reports; ..."

# Commit to Git (happens BEFORE exit)
git add -f infoblox_mvp1/robot_reports/pre_check/* || true
git add -f infoblox_mvp1/robot_reports/execution_counters/* || true

if ! git diff --cached --quiet; then
  if [ $robot_exit_code -ne 0 ]; then
    commit_msg="Pre-check FAILED: ..."
  else
    commit_msg="Pre-check PASSED: ..."
  fi

  git commit -m "$commit_msg"
  git push origin HEAD:main
fi

# NOW exit with failure (if tests failed)
if [ $robot_exit_code -ne 0 ]; then
  exit 1  # â† Pipeline stops, but reports already committed!
fi
```

**Key insight:** Reports are generated and committed **BEFORE** the `exit 1` command.

---

## Benefits of This Approach

### âœ… 1. Failure Visibility
- Every test failure is tracked
- Historical failure patterns visible
- Can identify flaky tests

### âœ… 2. Audit Trail
- Complete record of all test runs
- Pass/fail history preserved
- Compliance requirements met

### âœ… 3. Debugging Support
- Detailed logs available even for failed runs
- Can investigate failures weeks later
- No data loss

### âœ… 4. Safety First
- Pipeline stops on pre-check failure (won't deploy bad changes)
- But reports are preserved for analysis
- Best of both worlds

### âœ… 5. Complete Statistics
- execution_summary.html shows ALL runs
- Includes both passes and failures
- Accurate execution counts

---

## Testing the Behavior

### How to Verify Failure Handling

1. **Manually fail a test:**
   ```robot
   *** Test Cases ***
   Test That Will Fail
       Should Be Equal    1    2    # Will fail!
   ```

2. **Run pipeline**

3. **Check Git commits:**
   ```bash
   git log --oneline | head -5
   ```

   Should see:
   ```
   abc123 Update execution statistics report [Pipeline: 12345]
   def456 Pre-check FAILED: cabgridmgr.amfam.com - a_record [Pipeline: 12345] [Total Executions: 48]
   ```

4. **Check repository files:**
   ```
   Repository â†’ infoblox_mvp1/robot_reports/
   â”œâ”€â”€ pre_check/combined_report.html (shows failure)
   â””â”€â”€ execution_summary.html (includes failure in stats)
   ```

---

## Summary Table

| Stage | On Failure | Reports Generated? | Reports Committed? | Pipeline Continues? |
|-------|-----------|-------------------|-------------------|-------------------|
| **pre_check** | Exit 1 | âœ… Yes | âœ… Yes | âŒ No (SAFE!) |
| **post_implementation** | Exit 1 | âœ… Yes | âœ… Yes | âŒ No (marks failed) |
| **generate_execution_report** | Never fails | âœ… Yes | âœ… Yes | âœ… Always runs |

---

## Common Questions

### Q: Will I see failure reports in Git?
**A:** âœ… YES! Reports are committed before the stage exits with failure.

### Q: Will the pipeline deploy if pre-check fails?
**A:** âŒ NO! Pipeline stops at pre-check failure (safe behavior).

### Q: Will execution_summary.html be generated if tests fail?
**A:** âœ… YES! The reporting stage has `when: always`, so it runs regardless.

### Q: Will failure statistics be tracked?
**A:** âœ… YES! ExecutionCounter tracks both passes and failures.

### Q: Can I see historical failures in combined_report.html?
**A:** âœ… YES! Failed runs are included in the merged history.

---

## Final Answer

### âœ… YES - Reports Generated Even on Failure!

**Every scenario covered:**
- âœ… Pre-check fails â†’ Reports committed to Git
- âœ… Post-check fails â†’ Reports committed to Git
- âœ… Both fail â†’ Both sets of reports committed
- âœ… Execution summary â†’ Always generated and committed

**Safety maintained:**
- â›” Pipeline stops on pre-check failure (won't deploy)
- â›” Pipeline marked as failed on post-check failure
- âœ… But all reports are preserved for analysis

**You will ALWAYS have failure reports in Git!** ğŸ“¦âœ…
