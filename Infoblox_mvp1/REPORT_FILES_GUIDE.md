# Complete Guide to Pre-Check and Post-Check Report Files

## Overview

The enhanced pipeline generates **multiple files** for both pre-check and post-check stages to provide comprehensive test reporting and historical tracking.

---

## File Structure

```
infoblox_mvp1/robot_reports/
│
├── pre_check/                              # Pre-check validation reports
│   ├── output.xml                          # Robot Framework XML output
│   ├── log.html                            # Detailed test execution log
│   ├── report.html                         # Current run HTML report
│   ├── combined_output.xml                 # Merged historical XML
│   ├── combined_log.html                   # Merged historical log
│   ├── combined_report.html                # Merged historical report ⭐
│   └── history/                            # Historical test runs
│       ├── output_20251001_100000.xml
│       ├── output_20251002_140000.xml
│       └── ... (up to 20 files)
│
├── post_check/                             # Post-implementation reports
│   ├── output.xml                          # Robot Framework XML output
│   ├── log.html                            # Detailed test execution log
│   ├── report.html                         # Current run HTML report
│   ├── combined_output.xml                 # Merged historical XML
│   ├── combined_log.html                   # Merged historical log
│   ├── combined_report.html                # Merged historical report ⭐
│   └── history/                            # Historical test runs
│       ├── output_20251001_110000.xml
│       ├── output_20251002_150000.xml
│       └── ... (up to 20 files)
│
├── execution_counters/                     # Test execution tracking
│   ├── pre_check_counter.json              # Pre-check execution counts
│   └── post_check_counter.json             # Post-check execution counts
│
└── execution_summary.html                  # Overall statistics dashboard
```

---

## Pre-Check Files (Detailed)

### 1. **output.xml** - Robot Framework Output

**Generated by**: Robot Framework (during test execution)
**Updated**: Every pipeline run
**Size**: Small to medium (depends on test count)

**Purpose**:
- Raw XML output from Robot Framework
- Contains all test results, timings, and status
- Machine-readable format for processing

**Contains**:
```xml
<robot>
  <suite name="Pre Check">
    <test name="Validate A Record">
      <status status="PASS" starttime="..." endtime="..."/>
    </test>
    <test name="Check Duplicates">
      <status status="PASS" starttime="..." endtime="..."/>
    </test>
  </suite>
</robot>
```

**Use when**:
- You need programmatic access to test results
- Integrating with other tools
- Debugging test execution

**View**: Text editor, XML viewer, or process with tools

---

### 2. **log.html** - Detailed Execution Log

**Generated by**: Robot Framework (during test execution)
**Updated**: Every pipeline run
**Size**: Medium to large (includes all test details)

**Purpose**:
- Detailed, step-by-step log of test execution
- Shows every keyword call, argument, and result
- Includes full error messages and stack traces

**Contains**:
```
Test: Validate A Record
  Keyword: Initialize Execution Counter
    Arguments: pre_check_counter.json
    Status: PASS

  Keyword: Make HTTP Request
    Arguments: GET, https://10.74.3.80/wapi/v2.13.4/record:a
    Response: 200 OK
    Status: PASS

  Keyword: Verify Record Exists
    Expected: test1.example.com
    Found: test1.example.com
    Status: PASS

Test Status: PASS (2.5s)
```

**Use when**:
- Debugging test failures (see exact failure point)
- Understanding what each test does
- Troubleshooting API calls or timeouts
- Finding which keyword failed

**View**: Open in web browser

**Best for**: Deep debugging of individual test failures

---

### 3. **report.html** - Current Run Summary ✅

**Generated by**: Robot Framework (during test execution)
**Updated**: Every pipeline run (new file each time)
**Size**: Small to medium

**Purpose**:
- Human-readable summary of **current run only**
- Shows pass/fail statistics
- Test execution times
- Quick overview of results

**Contains**:
```
Pre-Check Validation - a_record - Pipeline 12345

Summary
  Total: 5 tests
  Passed: 4 (80%)
  Failed: 1 (20%)
  Elapsed: 12.5s

Test Results
  ✅ Validate A Record - 2.1s - PASS
  ✅ Check Duplicates - 1.5s - PASS
  ❌ Verify Required Fields - 0.8s - FAIL
  ✅ Check IP Format - 0.8s - PASS
  ✅ Validate DNS View - 7.3s - PASS
```

**Use when**:
- Checking latest test run results
- Quick pass/fail verification
- Seeing current execution times
- **NOT for historical analysis**

**View**: Open in web browser

**Best for**: Immediate results of current pipeline run

---

### 4. **combined_output.xml** - Historical XML ⭐

**Generated by**: merge_reports.py (after Robot Framework completes)
**Updated**: Every pipeline run (merged from history)
**Size**: Large (combines up to 20 runs)

**Purpose**:
- Merged XML from last 20 test runs
- Contains all historical test data in one file
- Used by tools for trend analysis

**Contains**:
```xml
<robot>
  <suite name="Pre Check - Combined History">
    <!-- Run 1 -->
    <test name="Validate A Record">
      <status status="PASS" starttime="2025-10-01 10:00:00"/>
    </test>
    <!-- Run 2 -->
    <test name="Validate A Record">
      <status status="PASS" starttime="2025-10-02 14:00:00"/>
    </test>
    <!-- ... up to 20 runs -->
  </suite>
</robot>
```

**Use when**:
- Processing historical data programmatically
- Building custom reports or dashboards
- Analyzing trends with external tools

**View**: XML viewer, or process with Python/scripts

**Best for**: Programmatic access to test history

---

### 5. **combined_log.html** - Historical Detailed Log ⭐

**Generated by**: merge_reports.py
**Updated**: Every pipeline run
**Size**: Very large (all details from 20 runs)

**Purpose**:
- Detailed log of last 20 test runs combined
- Shows all keyword calls from all runs
- Complete execution history

**Contains**:
```
Run 1 (2025-10-01 10:00:00)
  Test: Validate A Record
    Keyword: Initialize Execution Counter
    Status: PASS
  Status: PASS

Run 2 (2025-10-02 14:00:00)
  Test: Validate A Record
    Keyword: Initialize Execution Counter
    Status: PASS
  Status: PASS

... (20 runs total)
```

**Use when**:
- Comparing how same test behaved across runs
- Finding when a test started failing
- Deep debugging of intermittent issues

**View**: Open in web browser (may be slow due to size)

**Best for**: Detailed historical debugging

---

### 6. **combined_report.html** - Historical Summary Report ⭐⭐⭐

**Generated by**: merge_reports.py
**Updated**: Every pipeline run
**Size**: Medium

**Purpose**:
- **MOST IMPORTANT FILE FOR HISTORY**
- Shows summary of last 20 runs merged together
- Displays test stability and trends
- Identifies flaky tests

**Contains**:
```
Pre Check - Combined History
Merged from 20 test executions
Time span: 2025-10-01 to 2025-10-08

Test Results (Historical View)
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
Test: Validate A Record
  Status: 20/20 PASS (100%)
  First run: 2025-10-01 09:00
  Last run: 2025-10-08 10:00
  Trend: ✅ Stable - Always passes

Test: Check Duplicates
  Status: 19/20 PASS (95%)
  First run: 2025-10-01 09:00
  Last run: 2025-10-08 10:00
  Failed: 1 time (2025-10-04 11:30)
  Trend: ⚠️ Mostly stable - One failure

Test: Verify Required Fields
  Status: 12/20 PASS (60%)
  First run: 2025-10-01 09:00
  Last run: 2025-10-08 10:00
  Failed: 8 times (intermittent)
  Trend: 🔴 FLAKY - Needs investigation!
  Recent failures:
    • 2025-10-08 10:00 ❌
    • 2025-10-07 14:30 ❌
    • 2025-10-06 09:15 ❌

Combined Statistics:
  Total executions: 100 (5 tests × 20 runs)
  Passed: 91 (91%)
  Failed: 9 (9%)
  Time span: 7 days
```

**Use when**:
- **Identifying flaky tests** (most important!)
- Seeing test stability over time
- Finding when a test started failing
- Measuring overall test health
- Compliance/audit reporting

**View**: Open in web browser

**Best for**: Understanding test trends and stability ⭐⭐⭐

---

### 7. **history/output_YYYYMMDD_HHMMSS.xml** - Individual Run Archives

**Generated by**: merge_reports.py (copies from output.xml)
**Updated**: New file added each pipeline run
**Size**: Small to medium per file
**Count**: Up to 20 files (oldest auto-deleted)

**Purpose**:
- Archive of each individual test run
- Raw data for combined reports
- Backup of historical executions

**Naming format**:
```
output_20251001_100000.xml  (2025-10-01 at 10:00:00)
output_20251002_140000.xml  (2025-10-02 at 14:00:00)
output_20251008_100000.xml  (2025-10-08 at 10:00:00)
```

**Contains**:
- Same format as output.xml
- One file per pipeline run
- Chronologically sorted

**Use when**:
- Need to see specific run from the past
- Comparing two specific runs
- Recovering data from a particular execution

**View**: XML viewer

**Best for**: Historical research of specific runs

---

## Post-Check Files (Same Structure)

Post-check generates the **exact same files** as pre-check, but for post-implementation verification tests:

```
post_check/
├── output.xml                    # Current run XML
├── log.html                      # Current run detailed log
├── report.html                   # Current run summary
├── combined_output.xml           # Last 20 runs XML
├── combined_log.html             # Last 20 runs log
├── combined_report.html          # Last 20 runs summary ⭐
└── history/
    └── output_*.xml              # Individual run archives
```

**Same purposes and uses as pre-check files**

---

## Execution Counter Files

### 1. **pre_check_counter.json**

**Generated by**: ExecutionCounter.py (Robot Framework library)
**Updated**: Every pipeline run
**Size**: Small (few KB)

**Purpose**:
- Tracks how many times each pre-check test has executed
- Records first and last run timestamps
- Maintains execution history

**Contains**:
```json
{
  "Validate A Record": {
    "count": 25,
    "first_run": "2025-10-01T10:00:00",
    "last_run": "2025-10-08T15:30:00",
    "history": [
      {
        "timestamp": "2025-10-08T15:30:00",
        "run_number": 25
      },
      {
        "timestamp": "2025-10-08T10:00:00",
        "run_number": 24
      }
      // ... (last 50 runs)
    ]
  },
  "Check Duplicates": {
    "count": 24,
    "first_run": "2025-10-01T10:00:00",
    "last_run": "2025-10-08T15:30:00",
    "history": [...]
  }
}
```

**Use when**:
- Checking how many times a test has run
- Finding when a test first executed
- Building custom statistics
- Tracking test usage

**View**: Text editor, JSON viewer, or Python

**Best for**: Test execution statistics

---

### 2. **post_check_counter.json**

**Same format as pre_check_counter.json**

Tracks post-implementation verification test executions.

---

## Overall Statistics File

### **execution_summary.html**

**Generated by**: generate_execution_report stage (Python script)
**Updated**: Every pipeline run
**Size**: Small to medium

**Purpose**:
- Overall dashboard of ALL test activity
- Combines pre-check and post-check statistics
- Shows total execution counts across all tests

**Contains**:
```html
🤖 Robot Framework Execution Report
Generated: 2025-10-08 15:30:00

Overall Statistics
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
Pre-Check Tests: 5
Pre-Check Executions: 125

Post-Check Tests: 5
Post-Check Executions: 120

Pre-Check Validation Tests
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
Test Name                    | Executions | First Run   | Last Run
Validate A Record            | 25         | 2025-10-01  | 2025-10-08
Check Duplicates             | 24         | 2025-10-01  | 2025-10-08
Verify Required Fields       | 25         | 2025-10-01  | 2025-10-08
Check IP Format              | 26         | 2025-10-01  | 2025-10-08
Validate DNS View            | 25         | 2025-10-01  | 2025-10-08

Post-Implementation Verification Tests
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
Test Name                    | Executions | First Run   | Last Run
Verify A Record Created      | 24         | 2025-10-01  | 2025-10-08
Verify DNS Resolution        | 24         | 2025-10-01  | 2025-10-08
Verify CNAME Created         | 24         | 2025-10-01  | 2025-10-08
Verify Network Exists        | 24         | 2025-10-01  | 2025-10-08
Verify DHCP Config           | 24         | 2025-10-01  | 2025-10-08
```

**Use when**:
- Getting overall view of all test activity
- Comparing pre-check vs post-check execution counts
- Finding most/least executed tests
- Generating management reports

**View**: Open in web browser

**Best for**: Executive summary of all testing activity

---

## File Generation Timeline

### During Pipeline Execution

```
Pipeline Run Started
│
├─ Pre-Check Stage
│  ├─ Robot Framework runs tests
│  │  ├─ Creates: output.xml
│  │  ├─ Creates: log.html
│  │  └─ Creates: report.html
│  │
│  ├─ ExecutionCounter updates
│  │  └─ Updates: pre_check_counter.json
│  │
│  └─ merge_reports.py runs
│     ├─ Copies: output.xml → history/output_TIMESTAMP.xml
│     ├─ Creates: combined_output.xml (merges last 20)
│     ├─ Creates: combined_log.html (merges last 20)
│     └─ Creates: combined_report.html (merges last 20) ⭐
│
├─ Deploy Stage
│  └─ (Ansible runs, no reports generated)
│
├─ Post-Implementation Stage
│  ├─ Robot Framework runs tests
│  │  ├─ Creates: output.xml
│  │  ├─ Creates: log.html
│  │  └─ Creates: report.html
│  │
│  ├─ ExecutionCounter updates
│  │  └─ Updates: post_check_counter.json
│  │
│  └─ merge_reports.py runs
│     ├─ Copies: output.xml → history/output_TIMESTAMP.xml
│     ├─ Creates: combined_output.xml (merges last 20)
│     ├─ Creates: combined_log.html (merges last 20)
│     └─ Creates: combined_report.html (merges last 20) ⭐
│
└─ Reporting Stage
   └─ Python script runs
      ├─ Reads: pre_check_counter.json
      ├─ Reads: post_check_counter.json
      └─ Creates: execution_summary.html
```

---

## Which File Should I Use?

### Quick Reference Guide

| Need | File to Use | Why |
|------|-------------|-----|
| **Latest test results** | `report.html` | Current run summary |
| **Test history/trends** ⭐ | `combined_report.html` | Last 20 runs merged |
| **Identify flaky tests** | `combined_report.html` | Shows pass/fail patterns |
| **Debug current failure** | `log.html` | Step-by-step execution |
| **Debug historical issue** | `combined_log.html` | All runs' details |
| **Overall statistics** | `execution_summary.html` | All tests, all time |
| **Execution counts** | `*_counter.json` | Raw count data |
| **Programmatic access** | `output.xml` or `combined_output.xml` | Machine-readable |
| **Specific past run** | `history/output_TIMESTAMP.xml` | Individual run archive |

---

## File Sizes (Approximate)

| File | Typical Size | Notes |
|------|--------------|-------|
| `output.xml` | 10-50 KB | Depends on test count |
| `log.html` | 50-500 KB | Detailed logs are larger |
| `report.html` | 20-100 KB | Summary only |
| `combined_output.xml` | 200 KB - 1 MB | 20 runs merged |
| `combined_log.html` | 1-10 MB | Can be large! |
| `combined_report.html` | 100-500 KB | Summary of 20 runs |
| `history/output_*.xml` | 10-50 KB each | Up to 20 files |
| `*_counter.json` | 1-10 KB | Small JSON file |
| `execution_summary.html` | 20-100 KB | Simple dashboard |

---

## Accessing Reports

### In GitLab UI (Artifacts)

1. Go to: `CI/CD → Pipelines`
2. Click pipeline number
3. Click job name (`pre_check` or `post_implementation`)
4. Click `Browse` (right sidebar)
5. Navigate to `infoblox_mvp1/robot_reports/`
6. Click file to download/view

### In Repository (After Commit)

1. Go to: `Repository → Files`
2. Navigate to: `infoblox_mvp1/robot_reports/`
3. Click on HTML files
4. Click `Download` or view raw

### Locally (After Git Pull)

```bash
git pull origin main

# View current pre-check run
open infoblox_mvp1/robot_reports/pre_check/report.html

# View pre-check HISTORY ⭐
open infoblox_mvp1/robot_reports/pre_check/combined_report.html

# View overall statistics
open infoblox_mvp1/robot_reports/execution_summary.html
```

---

## File Retention

### Artifacts (GitLab)
- **Pre-check/Post-check**: 1 week
- **Execution summary**: 1 month
- **Automatic cleanup**: After expiration

### Repository (Git)
- **All reports**: Committed and kept indefinitely
- **History files**: Last 20 runs (older auto-deleted)
- **Counters**: Persist forever (accumulate counts)

---

## Summary Table

| File | Generated By | Updated | Purpose | Best For |
|------|--------------|---------|---------|----------|
| `output.xml` | Robot | Every run | Raw test data | Automation |
| `log.html` | Robot | Every run | Detailed execution | Debugging failures |
| `report.html` | Robot | Every run | Current run summary | Latest results |
| `combined_output.xml` | merge_reports.py | Every run | Historical XML | Programmatic history |
| `combined_log.html` | merge_reports.py | Every run | Historical details | Deep debugging |
| `combined_report.html` ⭐ | merge_reports.py | Every run | Historical summary | **Test trends** |
| `history/*.xml` | merge_reports.py | Every run | Individual archives | Specific run lookup |
| `*_counter.json` | ExecutionCounter | Every run | Execution counts | Statistics |
| `execution_summary.html` | Python script | Every run | Overall dashboard | Big picture |

---

## Key Takeaways

### ⭐ Most Important Files

1. **`combined_report.html`** - Test history and trends (⭐⭐⭐)
2. **`report.html`** - Current run results
3. **`execution_summary.html`** - Overall statistics

### 📊 For Different Needs

- **Need current results?** → `report.html`
- **Need test history?** → `combined_report.html` ⭐
- **Need to debug?** → `log.html` (current) or `combined_log.html` (historical)
- **Need statistics?** → `execution_summary.html`
- **Need raw data?** → `*.xml` files

### 🎯 The Golden Rule

**Always check `combined_report.html` to understand test stability and trends!**

This file shows you if a test is:
- ✅ Stable (always passes)
- ⚠️ Mostly stable (occasional failures)
- 🔴 Flaky (frequent failures - needs fixing!)

---

**All files are generated automatically by the pipeline - no manual intervention needed!** 🎉
